# This is the uCore butane template file for a NextCloud server.
# It will be run through jinja2 expanding the following macros:
#     CORE_USER_SSH_PUB        - SSH public key for core admin account
#     CORE_USER_PW_HASH        - Hash of password for core admin account 
#     ZFS_DATASET_KEY_B64      - Base64 encoded encryption key shared by all ZFS datasets
#     ADMIN_EMAIL              - E-mail address for administrator, sent from account
#                                associated with GMAIL_APP_PW
#     GMAIL_APP_PW             - Application password configured for GMail account, used
#                                by ADMIN_EMAIL
#     DATADISKS                - Space separated list of device paths to be included into
#                                ZFS RAID-6 (ZRAID2) root dataset, which is then used
#                                by all other datasets setup for the quadlets
#     POSTGRESQL_PW            . Password used by NextCloud when storing its data in the
#                                PostgreSQL database
#     NEXTCLOUD_ADMIN_PW       - Password for administering NextCloud
#     REDIS_PW                 - Password used by NextCloud for Redis access
#     NEXTCLOUD_TRUSTED_DOMAIN - IP-address or hostname associated with NextCloud
#
# The following quadlets are being setup:
#     NextCloud    - Implements the main functionality
#     Redis        - ??
#     PostgreSQL   - Database for NextCloud meta-data storage
#     Caddy        - HTTP/HTTPS proxy for NextCloud, currently only support internal
#                    SSL certificate which can be downloaded at
#                    https://<NEXTCLOUD_TRUSTED_DOMAIN>/caddy/nextcloud.pem

variant: fcos
version: {{FCOS_VER}}

boot_device:
  luks:
    tpm2: true

passwd:
  users:
    - name: core
      groups: [ wheel ]
      ssh_authorized_keys:
        - {{CORE_USER_SSH_PUB}}
      password_hash: {{CORE_USER_PW_HASH}}
    - name: podman
      groups: [ "systemd-journal" ]
      no_create_home: false
      home_dir: /var/home/podman
      shell: /bin/bash

storage:
  directories:
    # Marker dir used by auto-rebase services (per uCore example flow)
    - path: /etc/ucore-autorebase
      mode: 0755
    # Enable user lingering for core (rootless systemd user services at boot)
    - path: /var/lib/systemd/linger
      mode: 0755

  links:
    # Place public certificate in a place accessible from web
    - path: /var/pods/caddy/var/www/nextcloud.crt
      target: srv/pods/caddy/pki/authorities/local/root.crt
      hard: false

  files:
    # Decryption key for ZFS datasets
    - path: /var/zfs/zfs-dataset-key
      mode: 0600
      overwrite: true
      contents:
        source: "data:;base64,{{ZFS_DATASET_KEY_B64}}"

    # Enable linger for 'podman' (user services at boot)
    - path: /var/lib/systemd/linger/podman
      mode: 0644
      contents: { inline: "" }

    # Setting up mail relay agent to use GMail with application password
    - path: /var/home/podman/.config/msmtp/config
      mode: 0600
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          defaults
          auth           on
          tls            on
          tls_trust_file /etc/ssl/certs/ca-bundle.crt
          logfile        /var/home/podman/msmtp.log

          account        gmail
          host           smtp.gmail.com
          port           587
          from           {{ADMIN_EMAIL}}
          user           {{ADMIN_EMAIL}}
          password       {{GMAIL_APP_PW}}
          account default : gmail

    # ZFS setup script (creates pool/datasets on first boot after rebase)
    - path: /usr/local/bin/zfs-setup
      mode: 0755
      contents:
        inline: |
          #!/usr/bin/env bash
          set -euxo pipefail
          POOL=data
          MARKER=/etc/zfs/.${POOL}_created

          DISKS=({{DATADISKS}})

          if [[ -e "${MARKER}" ]]; then exit 0; fi

          # Wait for ZFS tools (post-rebase to uCore) and block devices
          /usr/bin/modprobe zfs || true
          for d in "${DISKS[@]}"; do
            /usr/bin/udevadm settle --exit-if-exists="$d" || true
          done

          # Create raidz2 (RAID6-equivalent) pool, if not existing already
          # If it exists, script will assume it has been auto-mounted
          # Setting mountpoint to "none" for root dataset. This makes
          # child datasets independent, i.e. snapshots of child datasets
          # can be done independently.
          if ! /usr/sbin/zpool list "${POOL}" >/dev/null 2>&1; then
            /usr/sbin/zpool create -m none -f -o ashift=12 "${POOL}" raidz2 "${DISKS[@]}"
          fi

          # Base properties common to most datasets
          COMMON_OPTS="-o encryption=aes-256-gcm -o keyformat=raw -o keylocation=file:///var/zfs/zfs-dataset-key -o compression=lz4 -o atime=off -o xattr=sa -o acltype=posixacl"

          # Create datasets with sensible options
          # Nextcloud app data (webroot + data dir)
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/var/pods/nextcloud         "${POOL}/nextcloud"        || true

          # PostgreSQL data (8K records to match PG page size)
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/var/pods/postgres \
              -o recordsize=8K -o logbias=throughput "${POOL}/postgres"                 || true

          # Redis data (small records)
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/var/pods/redis \
              -o recordsize=4K "${POOL}/redis"                                          || true

          # Caddy config + sites + certs
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/var/pods/caddy            "${POOL}/caddy"             || true
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/var/pods/caddy/www        "${POOL}/caddy/www"         || true
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/var/pods/caddy/certs      "${POOL}/caddy/certs"       || true

          chown -R podman:podman /var/pods
          mkdir -p /etc/zfs && touch "${MARKER}"

    # Caddyfile served from ZFS dataset (reverse proxy to Nextcloud + static cert download)
    - path: /var/pods/caddy/Caddyfile
      mode: 0644
      contents:
        inline: |
          # HTTP â†’ HTTPS
          :80 {
            redir https://{host}{uri} 308
          }

          # HTTPS with your private cert/key mounted into /data/certs
          :443 {
            tls internal
            encode zstd gzip

            # Allow downloading the public cert at /caddy/nextcloud.crt
            handle_path /caddy/* {
              root * /var/www
              file_server browse
            }

            # Reverse proxy to Nextcloud container (apache)
            @wellknown path /.well-known/*

            handle @wellknown {
              # Nextcloud well-known redirects per docs
              rewrite /.well-known/carddav /remote.php/dav
              rewrite /.well-known/caldav /remote.php/dav
              reverse_proxy nextcloud:80
            }

            handle {
              header {
                Strict-Transport-Security "max-age=31536000; includeSubDomains; preload"
                X-Content-Type-Options "nosniff"
                X-Frame-Options "SAMEORIGIN"
                Referrer-Policy "no-referrer"
              }
              reverse_proxy nextcloud:80
            }
          }


    # Nextcloud environment
    - path: /var/home/podman/.config/containers/nextcloud.env
      mode: 0600
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          POSTGRES_DB=nextcloud
          POSTGRES_USER=nextcloud
          POSTGRES_PASSWORD={{POSTGRESQL_PW}}
          NEXTCLOUD_ADMIN_USER=admin
          NEXTCLOUD_ADMIN_PASSWORD={{NEXTCLOUD_ADMIN_PW}}
          REDIS_HOST=redis
          REDIS_HOST_PASSWORD={{REDIS_PW}}
          NEXTCLOUD_TRUSTED_DOMAINS={{NEXTCLOUD_TRUSTED_DOMAIN}}
          APACHE_DISABLE_REWRITE_IP=1

    # Postgres environment
    - path: /var/home/podman/.config/containers/postgres.env
      mode: 0600
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          POSTGRES_DB=nextcloud
          POSTGRES_USER=nextcloud
          POSTGRES_PASSWORD={{POSTGRESQL_PW}}

    # Redis environment
    - path: /var/home/podman/.config/containers/redis.env
      mode: 0600
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          REDIS_PASSWORD={{REDIS_PW}}

    # Shared network for all rootless quadlets
    - path: /var/home/podman/.config/containers/systemd/nextcloud.network
      mode: 0644
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          [Unit]
          Description=User network for Nextcloud stack

          [Network]
          Driver=bridge

    # PostgreSQL quadlet: Store NextCloud meta-data
    - path: /var/home/podman/.config/containers/systemd/postgres.container
      mode: 0644
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          [Unit]
          Description=PostgreSQL for Nextcloud (rootless)
          After=nextcloud-network.service
          Requires=nextcloud-network.service
          OnFailure=mail-notify@%n.service

          [Container]
          Image=docker.io/library/postgres:16
          Network=nextcloud
          EnvironmentFile=%h/.config/containers/postgres.env
          # Persist to ZFS dataset
          Volume=/var/pods/postgres:/var/lib/postgresql/data:Z
          # Healthcheck
          HealthCmd=CMD-SHELL pg_isready -U $$POSTGRES_USER || exit 1
          HealthInterval=30s
          HealthRetries=5
          HealthStartPeriod=40s

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # Redis quadlet: Used by NextCloud
    - path: /var/home/podman/.config/containers/systemd/redis.container
      mode: 0644
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          [Unit]
          Description=Redis for Nextcloud (rootless, with auth)
          After=nextcloud-network.service
          Requires=nextcloud-network.service
          OnFailure=mail-notify@%n.service

          [Container]
          Image=docker.io/library/redis:7
          Network=nextcloud
          EnvironmentFile=%h/.config/containers/redis.env
          Volume=/var/pods/redis:/data:Z
          # Arguments
          --appendonly
          yes
          --requirepass
          $${REDIS_PASSWORD}

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # NextCloud quadlet: Offers your own cloud services
    - path: /var/home/podman/.config/containers/systemd/nextcloud.container
      mode: 0644
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          [Unit]
          Description=Nextcloud (rootless)
          After=postgres.service redis.service nextcloud-network.service
          Requires=postgres.service redis.service nextcloud-network.service
          OnFailure=mail-notify@%n.service

          [Container]
          Image=docker.io/library/nextcloud:29-apache
          Network=nextcloud
          EnvironmentFile=%h/.config/containers/nextcloud.env
          # Persist Nextcloud data to ZFS
          Volume=/var/pods/nextcloud:/var/www/html:Z

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # Caddy quadlet: Web reverse proxy
    - path: /var/home/podman/.config/containers/systemd/caddy.container
      mode: 0644
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          [Unit]
          Description=Caddy reverse proxy (rootless, private cert)
          After=nextcloud.service nextcloud-network.service
          Requires=nextcloud-network.service
          OnFailure=mail-notify@%n.service

          [Container]
          Image=docker.io/library/caddy:2
          Network=nextcloud
          # Map ports 80/443 (rootlesskit handles privileged ports)
          PublishPort=80:80
          PublishPort=443:443
          Volume=/var/pods/caddy/Caddyfile:/etc/caddy/Caddyfile:Z
          Volume=/var/pods/caddy/www:/var/www:Z
          Volume=/var/pods/caddy/certs:/data/certs:Z

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # Systemd USER timer to run weekly PostgreSQL amcheck
    - path: /var/home/podman/.config/systemd/user/pg-amcheck.service
      mode: 0644
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          [Unit]
          Description=Weekly PostgreSQL amcheck (online checks)
          OnFailure=mail-notify@%n.service

          [Service]
          Type=oneshot
          # Run pg_amcheck inside the postgres container
          ExecStart="/usr/bin/podman exec -u postgres postgres pg_amcheck -S -j 2 -d nextcloud"

    - path: /var/home/podman/.config/systemd/user/pg-amcheck.timer
      mode: 0644
      user: { name: podman }
      group: { name: podman }
      contents:
        inline: |
          [Unit]
          Description=Run pg_amcheck weekly

          [Timer]
          OnCalendar=weekly
          Persistent=true

          [Install]
          WantedBy=timers.target

systemd:

  #
  # Privileged system wide containers
  #

  units:

    #
    # SystemD services handling setup of the system,
    # These services only run once.
    #

    # Run on first boot, rebase from CoreOS to unsigned uCore distro
    - name: ucore-unsigned-autorebase.service
      enabled: true
      contents: |
        [Unit]
        Description=uCore autorebase to unsigned OCI and reboot
        ConditionPathExists=!/etc/ucore-autorebase/unverified
        ConditionPathExists=!/etc/ucore-autorebase/signed
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        StandardOutput=journal+console
        ExecStart=/usr/bin/rpm-ostree rebase --bypass-driver ostree-unverified-registry:ghcr.io/ublue-os/ucore-minimal:stable
        ExecStart=/usr/bin/touch /etc/ucore-autorebase/unverified
        ExecStart=/usr/bin/systemctl disable ucore-unsigned-autorebase.service
        ExecStart=/usr/bin/systemctl reboot

        [Install]
        WantedBy=multi-user.target

    # Run on second boot, rebase from unsigned uCore to signed uCore distro
    # Also install any needed extra packages
    - name: ucore-signed-autorebase.service
      enabled: true
      contents: |
        [Unit]
        Description=uCore autorebase to signed OCI and reboot
        ConditionPathExists=/etc/ucore-autorebase/unverified
        ConditionPathExists=!/etc/ucore-autorebase/signed
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        StandardOutput=journal+console
        ExecStart=/usr/bin/rpm-ostree rebase --bypass-driver ostree-image-signed:docker://ghcr.io/ublue-os/ucore-minimal:stable
        ExecStart=/usr/bin/rpm-ostree install -y --allow-inactive msmtp
        ExecStart=/usr/bin/touch /etc/ucore-autorebase/signed
        ExecStart=/usr/bin/systemctl disable ucore-signed-autorebase.service
        ExecStart=/usr/bin/systemctl reboot

        [Install]
        WantedBy=multi-user.target

    # Run on third boot, enable firewall services, ZFS pool/datasets, start containers, timers
    - name: post-setup.service
      enabled: true
      contents: |
        [Unit]
        Description=Post-rebase setup (firewall, ZFS, Quadlet, timers, boot mail)
        ConditionPathExists=/etc/ucore-autorebase/unverified
        ConditionPathExists=/etc/ucore-autorebase/signed
        ConditionPathExists=!/etc/ucore-autorebase/setup
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        StandardOutput=journal+console

        # Make sure rootless quadlets starts without requiring podman to login
        ExecStart=/usr/bin/loginctl enable-linger podman

        ExecStart=/usr/bin/firewall-cmd --permanent --add-service=http --add-service=https
        ExecStart=/usr/bin/firewall-cmd --reload

        # Create ZFS pool + datasets if not present
        ExecStart=/usr/local/bin/zfs-setup

        # start podman user's units
        ExecStart=/usr/bin/runuser -l podman -c 'systemctl --user daemon-reload'
        ExecStart=/usr/bin/runuser -l podman -c 'systemctl --user enable --now nextcloud-network.service'
        ExecStart=/usr/bin/runuser -l podman -c 'systemctl --user enable --now postgres.service redis.service nextcloud.service caddy.service'
        ExecStart=/usr/bin/runuser -l podman -c 'systemctl --user enable --now pg-amcheck.timer'

        # enable user timers (failure notify is implicit via OnFailure on each)
        ExecStart=/usr/bin/runuser -l podman -c 'systemctl --user daemon-reload'

        # enable weekly ZFS scrub + Postgres validation timers
        ExecStart=/usr/bin/systemctl enable --now zfs-scrub@data.timer

        ExecStart=/usr/bin/touch /etc/ucore-autorebase/setup
        ExecStart=/usr/bin/systemctl disable post-setup.service

        [Install]
        WantedBy=multi-user.target

    #
    # SystemD services run once setup is completed.
    #

    # Service: Run a ZFS scrub, triggered by zfs-scrub.timer
    - name: zfs-scrub.service
      contents: |
        [Unit]
        Description=ZFS scrub of storage pool
        Wants=zfs.target
        After=zfs.target
        OnFailure=mail-notify@%n.service

        [Service]
        Type=oneshot
        ExecStart=/usr/sbin/zpool scrub tank

    # Timer: Run weekly (Sunday 02:00)
    - name: zfs-scrub.timer
      enabled: true
      contents: |
        [Unit]
        Description=Weekly ZFS scrub

        [Timer]
        OnCalendar=Sun *-*-* 02:00:00
        Persistent=true

        [Install]
        WantedBy=timers.target

    # Send e-mail notification at each reboot
    # This is both to indicate that e-mail notifications work, as well as informing about reboots
    - name: boot-mail.service
      enabled: true
      contents: |
        [Unit]
        Description=Send boot notification email
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo -e 'Subject: [NextCloud server] System $(hostname) booted at $(date)' | msmtp -t {{ADMIN_EMAIL}}"

        [Install]
        WantedBy=multi-user.target

    # Template for failure notifications
    # To use this template, add the following line to systemd service in unit section:
    #   OnFailure=mail-notify@%n.service
    - name: mail-notify@.service
      contents: |
        [Unit]
        Description=Send failure notification email for %i
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo -e 'Subject: [NextCloud server] Service %i failed on $(hostname) at $(date)' | msmtp -t {{ADMIN_EMAIL}}"

