# uCore minimal host with rootless Podman + ZFS (raidz2) + Nextcloud stack via Quadlet
# Transpile with: butane --strict --pretty -d . -o ucore.ign thisfile.bu
variant: fcos
version: 1.5.0

passwd:
  users:
    - name: core
      groups: [ wheel ]
      ssh_authorized_keys:
        - {{CORE_USER_SSH_PUB}}
      password_hash: {{CORE_USER_PW_HASH}}

storage:
  directories:
    # Marker dir used by auto-rebase services (per uCore example flow)
    - path: /etc/ucore-autorebase
      mode: 0755
    # Enable user lingering for core (rootless systemd user services at boot)
    - path: /var/lib/systemd/linger
      mode: 0755
  files:
    # Create the linger file for 'core'
    - path: /var/lib/systemd/linger/core
      mode: 0644
      contents: { inline: "" }

    # msmtp config (replace with your Gmail address + app password)
    - path: /etc/msmtprc
      mode: 0600
      contents:
        inline: |
          defaults
          auth           on
          tls            on
          tls_trust_file /etc/ssl/certs/ca-bundle.crt
          logfile        /var/log/msmtp.log

          account        gmail
          host           smtp.gmail.com
          port           587
          from           YOUR_ADDRESS@gmail.com
          user           YOUR_ADDRESS@gmail.com
          password       YOUR_GMAIL_APP_PASSWORD
          account default : gmail

    # ZFS setup script (creates pool/datasets on first boot after rebase)
    - path: /usr/local/bin/zfs-setup
      mode: 0755
      contents:
        inline: |
          #!/usr/bin/env bash
          set -euxo pipefail
          POOL=data
          MARKER=/etc/zfs/.${POOL}_created

          # Adjust these if your "disk 2–5" aren't /dev/sdb–/dev/sde
          DISKS=(/dev/sdb /dev/sdc /dev/sdd /dev/sde)

          if [[ -e "${MARKER}" ]]; then
            exit 0
          fi

          # Wait for ZFS tools (post-rebase to uCore) and block devices
          /usr/bin/modprobe zfs || true
          for d in "${DISKS[@]}"; do
            /usr/bin/udevadm settle --exit-if-exists="$d" || true
          done

          # Create raidz2 (RAID6-equivalent) pool
          if ! /usr/sbin/zpool list "${POOL}" >/dev/null 2>&1; then
            /usr/sbin/zpool create -f -o ashift=12 \
              "${POOL}" raidz2 "${DISKS[@]}"
          fi

          # Base properties common to most datasets
          COMMON_OPTS="-o compression=lz4 -o atime=off -o xattr=sa -o acltype=posixacl"

          # Create datasets with sensible options
          # Nextcloud app data (webroot + data dir)
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/srv/pods/nextcloud         "${POOL}/nextcloud"        || true

          # PostgreSQL data (8K records to match PG page size)
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/srv/pods/postgres \
              -o recordsize=8K -o logbias=throughput "${POOL}/postgres"                 || true

          # Redis data (small records)
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/srv/pods/redis \
              -o recordsize=4K "${POOL}/redis"                                          || true

          # Caddy config + sites + certs
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/srv/pods/caddy            "${POOL}/caddy"             || true
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/srv/pods/caddy/www        "${POOL}/caddy/www"         || true
          /usr/sbin/zfs create ${COMMON_OPTS} -o mountpoint=/srv/pods/caddy/certs      "${POOL}/caddy/certs"       || true

          # Placeholders for your private cert and key (Caddy mounts these)
          touch /srv/pods/caddy/certs/liljegren.crt
          touch /srv/pods/caddy/certs/liljegren.key
          chmod 0644 /srv/pods/caddy/certs/liljegren.crt
          chmod 0600 /srv/pods/caddy/certs/liljegren.key
          chown -R 1000:1000 /srv/pods

          # Mark complete
          mkdir -p /etc/zfs
          touch "${MARKER}"

    # Caddyfile served from ZFS dataset (reverse proxy to Nextcloud + static cert download)
    - path: /srv/pods/caddy/Caddyfile
      mode: 0644
      contents:
        inline: |
          # HTTP → HTTPS
          :80 {
            redir https://{host}{uri} 308
          }

          # HTTPS with your private cert/key mounted into /data/certs
          :443 {
            tls /data/certs/liljegren.crt /data/certs/liljegren.key
            encode zstd gzip

            # Allow downloading the public cert at /caddy/liljegren.crt
            handle_path /caddy/* {
              root * /var/www
              file_server browse
            }

            # Reverse proxy to Nextcloud container (apache)
            @wellknown path /.well-known/*

            handle @wellknown {
              # Nextcloud well-known redirects per docs
              rewrite /.well-known/carddav /remote.php/dav
              rewrite /.well-known/caldav /remote.php/dav
              reverse_proxy nextcloud:80
            }

            handle {
              header {
                Strict-Transport-Security "max-age=31536000; includeSubDomains; preload"
                X-Content-Type-Options "nosniff"
                X-Frame-Options "SAMEORIGIN"
                Referrer-Policy "no-referrer"
              }
              reverse_proxy nextcloud:80
            }
          }

    # Nextcloud environment
    - path: /var/home/core/.config/containers/nextcloud.env
      mode: 0600
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          POSTGRES_DB=nextcloud
          POSTGRES_USER=nextcloud
          POSTGRES_PASSWORD={{POSTGRESQL_PW}}
          NEXTCLOUD_ADMIN_USER=admin
          NEXTCLOUD_ADMIN_PASSWORD={{NEXTCLOUD_ADMIN_PW}}
          REDIS_HOST=redis
          REDIS_HOST_PASSWORD=
          NEXTCLOUD_TRUSTED_DOMAINS=your.domain.tld

    # Postgres environment
    - path: /var/home/core/.config/containers/postgres.env
      mode: 0600
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          POSTGRES_DB=nextcloud
          POSTGRES_USER=nextcloud
          POSTGRES_PASSWORD=CHANGE_ME_STRONG_DB_PASSWORD

    # Redis environment
    - path: /var/home/core/.config/containers/redis.env
      mode: 0600
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          REDIS_PASSWORD=CHANGE_ME_STRONG_REDIS_PASSWORD

    # Quadlet user units
    - path: /var/home/core/.config/containers/systemd/nextcloud.network
      mode: 0644
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          [Unit]
          Description=Rootless Podman user network for Nextcloud stack
          [Network]
          Name=nextcloudnet
          Driver=bridge

    # PostgreSQL (official image)
    - path: /var/home/core/.config/containers/systemd/postgres.container
      mode: 0644
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          [Unit]
          Description=PostgreSQL for Nextcloud (rootless)
          After=nextcloud.network
          Requires=nextcloud.network
          OnFailure=mail-notify@%n.service

          [Container]
          Image=postgres:16
          Name=postgres
          Network=nextcloudnet
          EnvironmentFile=%h/.config/containers/postgres.env
          # Persist to ZFS dataset
          Volume=/srv/pods/postgres:/var/lib/postgresql/data:Z
          # Healthcheck
          HealthCmd=CMD-SHELL pg_isready -U $$POSTGRES_USER || exit 1
          HealthInterval=30s
          HealthRetries=5
          HealthStartPeriod=40s

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # Redis (official image)
    - path: /var/home/core/.config/containers/systemd/redis.container
      mode: 0644
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          [Unit]
          Description=Redis for Nextcloud (rootless, with auth)
          After=nextcloud.network
          Requires=nextcloud.network
          OnFailure=mail-notify@%n.service

          [Container]
          Image=redis:7
          Name=redis
          Network=nextcloudnet
          EnvironmentFile=%h/.config/containers/redis.env
          Volume=/srv/pods/redis:/data:Z
          Arg=--appendonly yes
          Arg=--requirepass $${REDIS_PASSWORD}

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # Nextcloud (apache variant for simplicity)
    - path: /var/home/core/.config/containers/systemd/nextcloud.container
      mode: 0644
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          [Unit]
          Description=Nextcloud (rootless)
          After=postgres.service redis.service nextcloud.network
          Requires=postgres.service redis.service nextcloud.network
          OnFailure=mail-notify@%n.service

          [Container]
          Image=nextcloud:29-apache
          Name=nextcloud
          Network=nextcloudnet
          EnvironmentFile=%h/.config/containers/nextcloud.env
          # Persist Nextcloud data to ZFS
          Volume=/srv/pods/nextcloud:/var/www/html:Z

          # Let Nextcloud know the proxied protocol
          Env=APACHE_DISABLE_REWRITE_IP=1

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # Caddy reverse proxy
    - path: /var/home/core/.config/containers/systemd/caddy.container
      mode: 0644
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          [Unit]
          Description=Caddy reverse proxy for Nextcloud (rootless, private cert)
          After=nextcloud.service nextcloud.network
          Requires=nextcloud.network
          OnFailure=mail-notify@%n.service

          [Container]
          Image=caddy:2
          Name=caddy
          Network=nextcloudnet
          # Map ports 80/443 (rootlesskit handles privileged ports)
          PublishPort=80:80
          PublishPort=443:443
          Volume=/srv/pods/caddy/Caddyfile:/etc/caddy/Caddyfile:Z
          Volume=/srv/pods/caddy/www:/var/www:Z
          Volume=/srv/pods/caddy/certs:/data/certs:Z

          [Service]
          Restart=always

          [Install]
          WantedBy=default.target

    # Systemd USER timer to run weekly PostgreSQL amcheck
    - path: /var/home/core/.config/systemd/user/pg-amcheck.service
      mode: 0644
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          [Unit]
          Description=Weekly PostgreSQL amcheck (online checks)
          OnFailure=mail-notify@%n.service

          [Service]
          Type=oneshot
          # Run pg_amcheck inside the postgres container
          ExecStart=/usr/bin/podman exec -u postgres postgres pg_amcheck -S -j 2 -d nextcloud

    - path: /var/home/core/.config/systemd/user/pg-amcheck.timer
      mode: 0644
      user: { name: core }
      group: { name: core }
      contents:
        inline: |
          [Unit]
          Description=Run pg_amcheck weekly

          [Timer]
          OnCalendar=weekly
          Persistent=true

          [Install]
          WantedBy=timers.target

systemd:
  units:
    # --- uCore recommended auto-rebase (unsigned first, then signed), then reboot ---
    - name: ucore-unsigned-autorebase.service
      enabled: true
      contents: |
        [Unit]
        Description=uCore autorebase to unsigned OCI and reboot
        ConditionPathExists=!/etc/ucore-autorebase/unverified
        ConditionPathExists=!/etc/ucore-autorebase/signed
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        StandardOutput=journal+console
        ExecStart=/usr/bin/rpm-ostree rebase --bypass-driver ostree-unverified-registry:ghcr.io/ublue-os/ucore-minimal:stable
        ExecStart=/usr/bin/touch /etc/ucore-autorebase/unverified
        ExecStart=/usr/bin/systemctl disable ucore-unsigned-autorebase.service
        ExecStart=/usr/bin/systemctl reboot

        [Install]
        WantedBy=multi-user.target

    - name: ucore-signed-autorebase.service
      enabled: true
      contents: |
        [Unit]
        Description=uCore autorebase to signed OCI and reboot
        ConditionPathExists=/etc/ucore-autorebase/unverified
        ConditionPathExists=!/etc/ucore-autorebase/signed
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        StandardOutput=journal+console
        ExecStart=/usr/bin/rpm-ostree rebase --bypass-driver ostree-image-signed:docker://ghcr.io/ublue-os/ucore-minimal:stable
        ExecStart=/usr/bin/touch /etc/ucore-autorebase/signed
        ExecStart=/usr/bin/systemctl disable ucore-signed-autorebase.service
        ExecStart=/usr/bin/systemctl reboot

        [Install]
        WantedBy=multi-user.target

    # Post-rebase setup: enable firewall services, ZFS pool/datasets, start containers, timers
    - name: post-setup.service
      enabled: true
      contents: |
        [Unit]
        Description=Post-rebase setup (linger, firewall, ZFS, Quadlet, timers)
        ConditionPathExists=/etc/ucore-autorebase/unverified
        ConditionPathExists=/etc/ucore-autorebase/signed
        ConditionPathExists=!/etc/ucore-autorebase/setup
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        StandardOutput=journal+console

        # Rootless services at boot
        ExecStart=/usr/bin/loginctl enable-linger core

        # Open HTTP/HTTPS
        ExecStart=/usr/bin/firewall-cmd --permanent --add-service=http --add-service=https
        ExecStart=/usr/bin/firewall-cmd --reload

        # Create ZFS pool + datasets if not present
        ExecStart=/usr/local/bin/zfs-setup

        # Ensure the public cert is copied to web path for download
        ExecStart=/usr/bin/install -m 0644 -o core -g core /srv/pods/caddy/certs/liljegren.crt /srv/pods/caddy/www/liljegren.crt

        # Enable rootless Quadlet units (user scope)
        ExecStart=/usr/bin/runuser -l core -c 'systemctl --user daemon-reload'
        ExecStart=/usr/bin/runuser -l core -c 'systemctl --user enable --now nextcloud.network'
        ExecStart=/usr/bin/runuser -l core -c 'systemctl --user enable --now postgres.service redis.service nextcloud.service caddy.service'
        ExecStart=/usr/bin/runuser -l core -c 'systemctl --user enable --now pg-amcheck.timer'

        # Enable weekly ZFS scrub timer for the pool
        ExecStart=/usr/bin/systemctl enable --now zfs-scrub-weekly@data.timer

        ExecStart=/usr/bin/touch /etc/ucore-autorebase/setup
        ExecStart=/usr/bin/systemctl disable post-setup.service

        [Install]
        WantedBy=multi-user.target

    # Service: Run a ZFS scrub
    - name: zfs-scrub.service
      contents: |
        [Unit]
        Description=ZFS scrub of storage pool
        Wants=zfs.target
        After=zfs.target

        [Service]
        Type=oneshot
        ExecStart=/usr/sbin/zpool scrub tank

    # Timer: Run weekly (Sunday 02:00)
    - name: zfs-scrub.timer
      enabled: true
      contents: |
        [Unit]
        Description=Weekly ZFS scrub

        [Timer]
        OnCalendar=Sun *-*-* 02:00:00
        Persistent=true

        [Install]
        WantedBy=timers.target

    # Notify by email at boot
    - name: boot-mail.service
      enabled: true
      contents: |
        [Unit]
        Description=Send boot notification email
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 'System $(hostname) booted at $(date)' | msmtp -a default YOUR_ADDRESS@gmail.com"

        [Install]
        WantedBy=multi-user.target

    # Template for failure notifications
    - name: mail-notify@.service
      contents: |
        [Unit]
        Description=Send failure notification email for %i
        After=network-online.target
        Wants=network-online.target

        [Service]
        Type=oneshot
        ExecStart=/bin/sh -c "echo 'Service %i failed on $(hostname) at $(date)' | msmtp -a default YOUR_ADDRESS@gmail.com"

